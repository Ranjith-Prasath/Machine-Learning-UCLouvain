{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d7d8cb-e65e-477b-9e48-f667cb3e862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE, mutual_info_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc169b7-39f3-4711-b5ed-b1a42f014bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   SMURF HEART FAILURE PROJECT: PART 2 - NONLINEAR MODELS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"   SMURF HEART FAILURE PROJECT: PART 2 - NONLINEAR MODELS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bfbb4-59e0-4ddc-9e6c-25ab0f5b23c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Loading data and preprocessor...\n",
      "Training data shape: (1000, 18)\n",
      "Target variable shape: (1000,)\n",
      "\n",
      "[STEP 2] Analyzing feature importance...\n",
      "Total features after preprocessing: 18\n",
      "\n",
      "  Method 1: F-statistic (correlation-based)\n",
      "\n",
      "Top 10 features by F-score:\n",
      "                                     Feature     F_Score\n",
      "1                             blood pressure  765.187456\n",
      "3                                cholesterol  251.986169\n",
      "8                                     weight  211.753990\n",
      "11                              smurfin_dots  109.499163\n",
      "12  profession_administration and governance   60.449747\n",
      "0                                        age   51.110103\n",
      "9                               sarsaparilla   41.607918\n",
      "14                profession_food production   27.723959\n",
      "4                                 hemoglobin   26.540494\n",
      "17                       profession_services   20.069050\n",
      "\n",
      "  Method 2: Mutual Information\n",
      "\n",
      "Top 10 features by Mutual Information:\n",
      "                                     Feature  MI_Score\n",
      "1                             blood pressure  0.239401\n",
      "3                                cholesterol  0.104785\n",
      "8                                     weight  0.087483\n",
      "6                                  potassium  0.059731\n",
      "11                              smurfin_dots  0.043937\n",
      "7                                  vitamin D  0.042250\n",
      "12  profession_administration and governance  0.038705\n",
      "0                                        age  0.034279\n",
      "4                                 hemoglobin  0.018693\n",
      "17                       profession_services  0.018401\n",
      "\n",
      "  Method 3: Random Forest Feature Importance\n",
      "\n",
      "Top 10 features by Random Forest:\n",
      "          Feature  Importance\n",
      "1  blood pressure    0.574847\n",
      "4      hemoglobin    0.109892\n",
      "6       potassium    0.092022\n",
      "8          weight    0.038563\n",
      "0             age    0.036736\n",
      "3     cholesterol    0.032550\n",
      "5          height    0.023952\n",
      "7       vitamin D    0.022102\n",
      "2         calcium    0.019491\n",
      "9    sarsaparilla    0.017416\n",
      "\n",
      "[STEP 3] Performing feature selection...\n",
      "\n",
      "Selecting top 15 features based on Mutual Information...\n",
      "\n",
      "Selected features (15):\n",
      "  1. age\n",
      "  2. blood pressure\n",
      "  3. cholesterol\n",
      "  4. hemoglobin\n",
      "  5. potassium\n",
      "  6. vitamin D\n",
      "  7. weight\n",
      "  8. sarsaparilla\n",
      "  9. smurfin_dots\n",
      "  10. profession_administration and governance\n",
      "  11. profession_craftsmanship\n",
      "  12. profession_food production\n",
      "  13. profession_manufacturing\n",
      "  14. profession_resource extraction\n",
      "  15. profession_services\n",
      "\n",
      "[STEP 4] Comparing multiple nonlinear models...\n",
      "Using 5-fold cross-validation on training data...\n",
      "\n",
      "Running cross-validation for each model...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Cross-Val RMSE: 0.045810 (+/- 0.003497)\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "  Cross-Val RMSE: 0.044902 (+/- 0.004640)\n",
      "\n",
      "Evaluating K-Nearest Neighbors...\n",
      "  Cross-Val RMSE: 0.055837 (+/- 0.003025)\n",
      "\n",
      "Evaluating Support Vector Regression...\n",
      "  Cross-Val RMSE: 0.061103 (+/- 0.001964)\n",
      "\n",
      "Evaluating Neural Network...\n",
      "  Cross-Val RMSE: 0.056600 (+/- 0.002244)\n",
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY (on Training Data)\n",
      "======================================================================\n",
      "                    Model  Mean CV RMSE  Std CV RMSE\n",
      "        Gradient Boosting      0.044902     0.004640\n",
      "            Random Forest      0.045810     0.003497\n",
      "      K-Nearest Neighbors      0.055837     0.003025\n",
      "           Neural Network      0.056600     0.002244\n",
      "Support Vector Regression      0.061103     0.001964\n",
      "\n",
      "[STEP 5] Deep Hyperparameter Tuning for top 2 models (Random Forest, Gradient Boosting)...\n",
      "\n",
      "  Tuning Gradient Boosting with GridSearchCV (EXHAUSTIVE SEARCH)...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "    Best CV RMSE: 0.043130\n",
      "    Best parameters: {'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 350, 'subsample': 0.75}\n",
      "\n",
      "  Tuning Random Forest with GridSearchCV (EXHAUSTIVE SEARCH)...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD DATA AND PREPROCESSOR\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Loading data and preprocessor...\")\n",
    "# Load the preprocessor from Part 1\n",
    "preprocessor = joblib.load('smurf_preprocessor.joblib')\n",
    "\n",
    "# Load training data\n",
    "df_X_train = pd.read_csv(\"X_train.csv\")\n",
    "\n",
    "df_y_train = pd.read_csv(\"y_train.csv\", header=None, names=['heart_failure_risk'])\n",
    "\n",
    "y_train = df_y_train['heart_failure_risk'].values\n",
    "\n",
    "# Apply same column renaming as Part 1\n",
    "df_X_train_renamed = df_X_train.rename(columns={\n",
    "    'smurfberry liquor': 'smurfberry_liquor',\n",
    "    'smurfin donuts': 'smurfin_dots'\n",
    "})\n",
    "# Transform training data using fitted preprocessor\n",
    "\n",
    "X_train_processed = preprocessor.transform(df_X_train_renamed)\n",
    "\n",
    "print(f\"Training data shape: {X_train_processed.shape}\")\n",
    "\n",
    "print(f\"Target variable shape: {y_train.shape}\")\n",
    "\n",
    "# Numerical feature names\n",
    "\n",
    "numerical_features = ['age', 'blood pressure', 'calcium', 'cholesterol', 'hemoglobin', \n",
    "\n",
    "                      'height', 'potassium', 'vitamin D', 'weight']\n",
    "\n",
    "processed_feature_names = (\n",
    "    preprocessor.named_transformers_['num'].get_feature_names_out().tolist() +\n",
    "    preprocessor.named_transformers_['ord'].get_feature_names_out().tolist() +\n",
    "    preprocessor.named_transformers_['nom'].get_feature_names_out().tolist()\n",
    ")\n",
    "X_train_standardized = X_train_processed.copy()\n",
    "\n",
    "scX = StandardScaler()\n",
    "\n",
    "X_train_standardized[:, numerical_indices] = scX.fit_transform(\n",
    "    X_train_processed[:, numerical_indices]\n",
    ")\n",
    "\n",
    "X_train_processed = X_train_standardized\n",
    "\n",
    "\n",
    "print(\"\\n[STEP 2] Analyzing feature importance...\")\n",
    "\n",
    "feature_names = (\n",
    "    preprocessor.named_transformers_['num'].get_feature_names_out().tolist() +\n",
    "    preprocessor.named_transformers_['ord'].get_feature_names_out().tolist() +\n",
    "    preprocessor.named_transformers_['nom'].get_feature_names_out().tolist()\n",
    ")\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(feature_names)}\")\n",
    "\n",
    "# 1: Correlation-based (F-statistic)\n",
    "print(\"\\n  Method 1: F-statistic (correlation-based)\")\n",
    "selector_f = SelectKBest(score_func=f_regression, k='all')\n",
    "selector_f.fit(X_train_processed, y_train)\n",
    "f_scores = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'F_Score': selector_f.scores_\n",
    "}).sort_values('F_Score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by F-score:\")\n",
    "print(f_scores.head(10))\n",
    "\n",
    "#2: Mutual Information (using it to capture NL relationships)\n",
    "print(\"\\n  Method 2: Mutual Information\")\n",
    "mi_scores = mutual_info_regression(X_train_processed, y_train, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by Mutual Information:\")\n",
    "print(mi_df.head(10))\n",
    "\n",
    "#3: Random Forest Feature Importance\n",
    "print(\"\\n  Method 3: Random Forest Feature Importance\")\n",
    "rf_temp = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_temp.fit(X_train_processed, y_train)\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_temp.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by Random Forest:\")\n",
    "print(rf_importance.head(10))\n",
    "\n",
    "# Save feature importance analysis\n",
    "feature_analysis = {\n",
    "    'f_scores': f_scores,\n",
    "    'mi_scores': mi_df,\n",
    "    'rf_importance': rf_importance\n",
    "}\n",
    "joblib.dump(feature_analysis, 'feature_importance_analysis.joblib')\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: FEATURE SELECTION\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 3] Performing feature selection...\")\n",
    "\n",
    "#  trying different numbers of features\n",
    "n_features_to_try = [10, 15, 20, 'all']\n",
    "\n",
    "K_FEATURES = 15  # You can adjust this\n",
    "print(f\"\\nSelecting top {K_FEATURES} features based on Mutual Information...\")\n",
    "\n",
    "selector_mi = SelectKBest(score_func=mutual_info_regression, k=K_FEATURES)\n",
    "X_train_selected = selector_mi.fit_transform(X_train_processed, y_train)\n",
    "\n",
    "selected_features = [feature_names[i] for i in selector_mi.get_support(indices=True)]\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(\"\\n[STEP 4] Comparing multiple nonlinear models...\")\n",
    "print(\"Using 5-fold cross-validation on training data...\")\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Support Vector Regression': SVR(),\n",
    "    'Neural Network': MLPRegressor(activation='logistic',random_state=42, max_iter=2000)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "print(\"\\nRunning cross-validation for each model...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_selected, y_train,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rmse_scores = np.sqrt(-cv_scores)\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean_rmse': rmse_scores.mean(),\n",
    "        'std_rmse': rmse_scores.std(),\n",
    "        'scores': rmse_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  Cross-Val RMSE: {rmse_scores.mean():.6f} (+/- {rmse_scores.std():.6f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-VALIDATION RESULTS SUMMARY (on Training Data)\")\n",
    "print(\"=\"*70)\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(cv_results.keys()),\n",
    "    'Mean CV RMSE': [cv_results[m]['mean_rmse'] for m in cv_results.keys()],\n",
    "    'Std CV RMSE': [cv_results[m]['std_rmse'] for m in cv_results.keys()]\n",
    "}).sort_values('Mean CV RMSE')\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n[STEP 5] Deep Hyperparameter Tuning for top 2 models (Random Forest, Gradient Boosting)...\")\n",
    "\n",
    "# Selecting  top 2 models for deep tuning -> cause the both models gave good rsults consistently.\n",
    "top_2_models = results_df.head(2)['Model'].tolist()\n",
    "\n",
    "# Define models and use GridSearchCV (full search) for the top model\n",
    "tuned_models = {}\n",
    "\n",
    "\n",
    "param_grid_rf_dense = {\n",
    "    'n_estimators': [250, 350, 450], # More trees for better averaging\n",
    "    'max_depth': [15, 20, None],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_gb_dense = {\n",
    "    'n_estimators': [250,350],\n",
    "    'learning_rate': [0.03, 0.05],\n",
    "    'max_depth': [2, 3],\n",
    "    'subsample': [0.75, 0.8]\n",
    "}\n",
    "\n",
    "tuning_configs = {\n",
    "    'Random Forest': (RandomForestRegressor(random_state=42, n_jobs=-1), param_grid_rf_dense, GridSearchCV),\n",
    "    'Gradient Boosting': (GradientBoostingRegressor(random_state=42), param_grid_gb_dense, GridSearchCV),\n",
    "}\n",
    "\n",
    "\n",
    "for model_name in top_2_models:\n",
    "    if model_name in tuning_configs:\n",
    "        model, param_grid, search_method = tuning_configs[model_name]\n",
    "        \n",
    "        print(f\"\\n  Tuning {model_name} with {search_method.__name__} (EXHAUSTIVE SEARCH)...\")\n",
    "        \n",
    "        search = search_method(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1, # Maximize speed\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        search.fit(X_train_selected, y_train)\n",
    "        tuned_models[model_name] = {\n",
    "            'model': search.best_estimator_,\n",
    "            'best_params': search.best_params_,\n",
    "            'best_rmse': np.sqrt(-search.best_score_)\n",
    "        }\n",
    "        print(f\"    Best CV RMSE: {tuned_models[model_name]['best_rmse']:.6f}\")\n",
    "        print(f\"    Best parameters: {search.best_params_}\")\n",
    "\n",
    "print(\"\\n[STEP 6] Selecting the Absolute Best Model...\")\n",
    "\n",
    "best_model_name = min(tuned_models.keys(), \n",
    "                      key=lambda x: tuned_models[x]['best_rmse'])\n",
    "best_model = tuned_models[best_model_name]['model']\n",
    "\n",
    "print(f\"\\n*** ABSOLUTE BEST MODEL: {best_model_name} ***\")\n",
    "print(f\"BEST CV RMSE: {tuned_models[best_model_name]['best_rmse']:.6f}\")\n",
    "print(f\"Best parameters: {tuned_models[best_model_name]['best_params']}\")\n",
    "\n",
    "print(\"\\nTraining best model on full training set...\")\n",
    "best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_train_pred = best_model.predict(X_train_selected)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nTraining Set Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.6f}\")\n",
    "print(f\"  R²:   {train_r2:.6f}\")\n",
    "print(f\"  MAE:  {train_mae:.6f}\")\n",
    "\n",
    "print(\"\\n[STEP 7] Final evaluation on test set...\")\n",
    "\n",
    "df_X_test = pd.read_csv(\"X_test.csv\")\n",
    "df_y_test = pd.read_csv(\"y_test.csv\", header=None, \n",
    "                        names=['heart_failure_risk'])\n",
    "y_test = df_y_test['heart_failure_risk'].values\n",
    "\n",
    "df_X_test_renamed = df_X_test.rename(columns={\n",
    "    'smurfberry liquor': 'smurfberry_liquor',\n",
    "    'smurfin donuts': 'smurfin_dots'\n",
    "})\n",
    "\n",
    "\n",
    "X_test_processed = preprocessor.transform(df_X_test_renamed)\n",
    "\n",
    "X_test_standardized = X_test_processed.copy()\n",
    "X_test_standardized[:, numerical_indices] = scX.transform(\n",
    "    X_test_processed[:, numerical_indices]\n",
    ")\n",
    "\n",
    "\n",
    "X_test_selected = selector_mi.transform(X_test_standardized)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  R²:   {test_r2:.6f}\")\n",
    "print(f\"  MAE:  {test_mae:.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 8\n",
    "print(\"\\n[STEP 8] Saving results and models...\")\n",
    "joblib.dump(selector_mi, 'feature_selector.joblib')\n",
    "joblib.dump(best_model, 'best_nonlinear_model.joblib')\n",
    "\n",
    "results_summary = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'best_params': tuned_models[best_model_name]['best_params'],\n",
    "    'selected_features': selected_features,\n",
    "    'cv_rmse': tuned_models[best_model_name]['best_rmse'],\n",
    "    'train_rmse': train_rmse,\n",
    "    'train_r2': train_r2,\n",
    "    'train_mae': train_mae,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_r2': test_r2,\n",
    "    'test_mae': test_mae,\n",
    "    'all_cv_results': cv_results,\n",
    "    'tuned_models': {k: {'params': v['best_params'], 'rmse': v['best_rmse']} \n",
    "                     for k, v in tuned_models.items()}\n",
    "}\n",
    "\n",
    "joblib.dump(results_summary, 'part2_results_summary.joblib')\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - feature_selector.joblib\")\n",
    "print(\"  - best_nonlinear_model.joblib\")\n",
    "print(\"  - part2_results_summary.joblib\")\n",
    "print(\"  - feature_importance_analysis.joblib\")\n",
    "\n",
    "# STEP 9\n",
    "print(\"\\n[STEP 9] Generating visualizations...\")\n",
    "\n",
    "# 1. Feature importance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# F-score\n",
    "axes[0].barh(f_scores.head(10)['Feature'], f_scores.head(10)['F_Score'])\n",
    "axes[0].set_xlabel('F-Score')\n",
    "axes[0].set_title('Top 10 Features: F-Statistic')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Mutual Information\n",
    "axes[1].barh(mi_df.head(10)['Feature'], mi_df.head(10)['MI_Score'])\n",
    "axes[1].set_xlabel('MI Score')\n",
    "axes[1].set_title('Top 10 Features: Mutual Information')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Random Forest\n",
    "axes[2].barh(rf_importance.head(10)['Feature'], rf_importance.head(10)['Importance'])\n",
    "axes[2].set_xlabel('Importance')\n",
    "axes[2].set_title('Top 10 Features: Random Forest')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: feature_importance_comparison.png\")\n",
    "\n",
    "# Model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = results_df['Model']\n",
    "cv_rmses = results_df['Mean CV RMSE']\n",
    "cv_stds = results_df['Std CV RMSE']\n",
    "\n",
    "plt.barh(model_names, cv_rmses, xerr=cv_stds, capsize=5)\n",
    "plt.xlabel('Cross-Validation RMSE')\n",
    "plt.title('Model Comparison (5-Fold Cross-Validation)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Predictions vs Actual (Test Set)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5, edgecolors='k', linewidths=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Heart Failure Risk')\n",
    "plt.ylabel('Predicted Heart Failure Risk')\n",
    "plt.title(f'Predictions vs Actual ({best_model_name}) - Test Set')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "# 4. Residuals plot\n",
    "residuals = y_test - y_test_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.5, edgecolors='k', linewidths=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(f'Residual Plot ({best_model_name}) - Test Set')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('residuals_plot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb53d2-bd59-4bc5-b0d6-31e3f271288d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89ba39-832b-4ade-a1ca-497f4a7fbab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
