{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f129f03-15fd-4803-8145-7b2ba3032298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   SMURF HEART FAILURE PROJECT: PART 3 - TREE-BASED MODELS (DT, RF, XGB)\n",
      "======================================================================\n",
      "\n",
      "[STEP 1] Loading data, preprocessor, and feature selector...\n",
      "Target variable y_train has been transformed using log(1+y).\n",
      "Selected training data shape: (1000, 15)\n",
      "\n",
      "[STEP 2] Comparing Tree-based models...\n",
      "Using 5-fold cross-validation on training data...\n",
      "\n",
      "Running cross-validation for each model...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "  Cross-Val RMSE: 0.052686 (+/- 0.002319)\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Cross-Val RMSE: 0.039560 (+/- 0.002490)\n",
      "\n",
      "Evaluating Gradient Boosting (SKL)...\n",
      "  Cross-Val RMSE: 0.037997 (+/- 0.003005)\n",
      "\n",
      "Evaluating XGBoost...\n",
      "  Cross-Val RMSE: 0.041265 (+/- 0.002344)\n",
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY (on Transformed Training Data)\n",
      "======================================================================\n",
      "                  Model  Mean CV RMSE  Std CV RMSE\n",
      "Gradient Boosting (SKL)      0.037997     0.003005\n",
      "          Random Forest      0.039560     0.002490\n",
      "                XGBoost      0.041265     0.002344\n",
      "          Decision Tree      0.052686     0.002319\n",
      "\n",
      "[STEP 3] Hyperparameter tuning for top 3 models...\n",
      "\n",
      "Tuning hyperparameters for: ['Gradient Boosting (SKL)', 'Random Forest', 'XGBoost']\n",
      "\n",
      "  Tuning Gradient Boosting (SKL)...\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 179\u001b[0m\n\u001b[0;32m    166\u001b[0m model_to_tune \u001b[38;5;241m=\u001b[39m models_tree[model_name]\n\u001b[0;32m    168\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    169\u001b[0m     model_to_tune,\n\u001b[0;32m    170\u001b[0m     param_grids_tree[model_name],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    177\u001b[0m )\n\u001b[1;32m--> 179\u001b[0m search\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_transformed)\n\u001b[0;32m    181\u001b[0m tuned_models[model_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: search\u001b[38;5;241m.\u001b[39mbest_estimator_,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: search\u001b[38;5;241m.\u001b[39mbest_params_,\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39msearch\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[0;32m    185\u001b[0m }\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Best CV RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtuned_models[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1960\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1960\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1961\u001b[0m         ParameterSampler(\n\u001b[0;32m   1962\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1963\u001b[0m         )\n\u001b[0;32m   1964\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    966\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    967\u001b[0m         clone(base_estimator),\n\u001b[0;32m    968\u001b[0m         X,\n\u001b[0;32m    969\u001b[0m         y,\n\u001b[0;32m    970\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    971\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    972\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    973\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    974\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    975\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    976\u001b[0m     )\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    979\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    981\u001b[0m )\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor # Highly popular and efficient GBDT\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "import warnings\n",
    "\n",
    "# Ignore minor warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"   SMURF HEART FAILURE PROJECT: PART 3 - TREE-BASED MODELS (DT, RF, XGB)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define the log-transform function (np.log1p) for y for variance stabilization\n",
    "LOG_TRANSFORM = True \n",
    "K_FEATURES = 15 # Number of features to select\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD DATA, PREPROCESSOR, AND FEATURE SELECTOR\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Loading data, preprocessor, and feature selector...\")\n",
    "\n",
    "# Load the preprocessor and feature selector from previous parts\n",
    "preprocessor = joblib.load('smurf_preprocessor.joblib')\n",
    "selector_mi = joblib.load('feature_selector.joblib')\n",
    "\n",
    "# Load training data\n",
    "df_X_train = pd.read_csv(\"X_train.csv\")\n",
    "df_y_train = pd.read_csv(\"y_train.csv\", header=None, names=['heart_failure_risk'])\n",
    "y_train = df_y_train['heart_failure_risk'].values\n",
    "\n",
    "# Apply same column renaming\n",
    "df_X_train_renamed = df_X_train.rename(columns={\n",
    "    'smurfberry liquor': 'smurfberry_liquor',\n",
    "    'smurfin donuts': 'smurfin_dots'\n",
    "})\n",
    "\n",
    "# Transform training data using fitted preprocessor\n",
    "X_train_processed = preprocessor.transform(df_X_train_renamed)\n",
    "\n",
    "# Select features using the loaded selector\n",
    "X_train_selected = selector_mi.transform(X_train_processed)\n",
    "\n",
    "# --- TARGET VARIABLE TRANSFORMATION ---\n",
    "if LOG_TRANSFORM:\n",
    "    y_train_transformed = np.log1p(y_train)\n",
    "    print(\"Target variable y_train has been transformed using log(1+y).\")\n",
    "else:\n",
    "    y_train_transformed = y_train\n",
    "\n",
    "print(f\"Selected training data shape: {X_train_selected.shape}\")\n",
    "\n",
    "# Get selected feature names (for plotting/reporting)\n",
    "feature_names = (\n",
    "    preprocessor.named_transformers_['num'].get_feature_names_out().tolist() +\n",
    "    preprocessor.named_transformers_['ord'].get_feature_names_out().tolist() +\n",
    "    preprocessor.named_transformers_['nom'].get_feature_names_out().tolist()\n",
    ")\n",
    "selected_features = [feature_names[i] for i in selector_mi.get_support(indices=True)]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: MODEL COMPARISON (TREE MODELS)\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 2] Comparing Tree-based models...\")\n",
    "print(\"Using 5-fold cross-validation on training data...\")\n",
    "\n",
    "# Define models to compare\n",
    "models_tree = {\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting (SKL)': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "cv_results = {}\n",
    "\n",
    "print(\"\\nRunning cross-validation for each model...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, model in models_tree.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_selected, y_train_transformed,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    # Convert to RMSE\n",
    "    rmse_scores = np.sqrt(-cv_scores)\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean_rmse': rmse_scores.mean(),\n",
    "        'std_rmse': rmse_scores.std(),\n",
    "        'scores': rmse_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  Cross-Val RMSE: {rmse_scores.mean():.6f} (+/- {rmse_scores.std():.6f})\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-VALIDATION RESULTS SUMMARY (on Transformed Training Data)\")\n",
    "print(\"=\"*70)\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(cv_results.keys()),\n",
    "    'Mean CV RMSE': [cv_results[m]['mean_rmse'] for m in cv_results.keys()],\n",
    "    'Std CV RMSE': [cv_results[m]['std_rmse'] for m in cv_results.keys()]\n",
    "}).sort_values('Mean CV RMSE')\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: HYPERPARAMETER TUNING FOR TOP MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 3] Hyperparameter tuning for top 3 models...\")\n",
    "\n",
    "# Select top 3 models\n",
    "top_3_models = results_df.head(3)['Model'].tolist()\n",
    "print(f\"\\nTuning hyperparameters for: {top_3_models}\")\n",
    "\n",
    "tuned_models = {}\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grids_tree = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting (SKL)': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.9, 1.0]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    print(f\"\\n  Tuning {model_name}...\")\n",
    "    \n",
    "    # Use RandomizedSearchCV for faster tuning\n",
    "    model_to_tune = models_tree[model_name]\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        model_to_tune,\n",
    "        param_grids_tree[model_name],\n",
    "        n_iter=20, # Try 20 random combinations\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, # Use all cores\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train_selected, y_train_transformed)\n",
    "    \n",
    "    tuned_models[model_name] = {\n",
    "        'model': search.best_estimator_,\n",
    "        'best_params': search.best_params_,\n",
    "        'best_rmse': np.sqrt(-search.best_score_)\n",
    "    }\n",
    "    \n",
    "    print(f\"    Best CV RMSE: {tuned_models[model_name]['best_rmse']:.6f}\")\n",
    "    print(f\"    Best parameters: {search.best_params_}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: SELECT BEST MODEL AND TRAIN ON FULL TRAINING SET\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 4] Selecting best model...\")\n",
    "\n",
    "# Find the best tuned model\n",
    "best_model_name = min(tuned_models.keys(), \n",
    "                      key=lambda x: tuned_models[x]['best_rmse'])\n",
    "best_model = tuned_models[best_model_name]['model']\n",
    "\n",
    "print(f\"\\n*** BEST TREE-BASED MODEL: {best_model_name} ***\")\n",
    "print(f\"CV RMSE (Transformed): {tuned_models[best_model_name]['best_rmse']:.6f}\")\n",
    "\n",
    "# Train on full training set\n",
    "print(\"\\nTraining best model on full training set (transformed y)...\")\n",
    "best_model.fit(X_train_selected, y_train_transformed)\n",
    "\n",
    "# Evaluate on training set\n",
    "y_train_pred_transformed = best_model.predict(X_train_selected)\n",
    "# --- INVERSE TRANSFORM FOR METRICS ---\n",
    "y_train_pred = np.expm1(y_train_pred_transformed) \n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nTraining Set Performance (Original Scale):\")\n",
    "print(f\"  RMSE: {train_rmse:.6f}\")\n",
    "print(f\"  R²:   {train_r2:.6f}\")\n",
    "print(f\"  MAE:  {train_mae:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: EVALUATE ON TEST SET (FINAL EVALUATION)\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 5] Final evaluation on test set...\")\n",
    "\n",
    "# Load test data\n",
    "df_X_test = pd.read_csv(\"X_test.csv\")\n",
    "df_y_test = pd.read_csv(\"y_test.csv\", header=None, \n",
    "                        names=['heart_failure_risk'])\n",
    "y_test = df_y_test['heart_failure_risk'].values\n",
    "\n",
    "# Apply same preprocessing\n",
    "df_X_test_renamed = df_X_test.rename(columns={\n",
    "    'smurfberry liquor': 'smurfberry_liquor',\n",
    "    'smurfin donuts': 'smurfin_dots'\n",
    "})\n",
    "\n",
    "X_test_processed = preprocessor.transform(df_X_test_renamed)\n",
    "X_test_selected = selector_mi.transform(X_test_processed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred_transformed = best_model.predict(X_test_selected)\n",
    "# --- INVERSE TRANSFORM PREDICTIONS ---\n",
    "y_test_pred = np.expm1(y_test_pred_transformed)\n",
    "\n",
    "# Calculate metrics (using original y_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"             FINAL TEST SET PERFORMANCE (Original Scale)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  R²:   {test_r2:.6f}\")\n",
    "print(f\"  MAE:  {test_mae:.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SAVE MODEL AND RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 6] Saving the best model and results...\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_tree_model.joblib')\n",
    "\n",
    "# Save comprehensive results\n",
    "results_summary = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'cv_rmse_transformed': tuned_models[best_model_name]['best_rmse'],\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_r2': test_r2,\n",
    "    'test_mae': test_mae,\n",
    "    'tuned_models': {k: {'params': v['best_params'], 'rmse': v['best_rmse']} for k, v in tuned_models.items()}\n",
    "}\n",
    "joblib.dump(results_summary, 'part3_results_summary.joblib')\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - best_tree_model.joblib\")\n",
    "print(\"  - part3_results_summary.joblib\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: GENERATE VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 7] Generating visualizations...\")\n",
    "\n",
    "# 1. Model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(results_df['Model'], results_df['Mean CV RMSE'], xerr=results_df['Std CV RMSE'], capsize=5)\n",
    "plt.xlabel('Cross-Validation RMSE (Transformed y)')\n",
    "plt.title('Tree Model Comparison (5-Fold Cross-Validation)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tree_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: tree_model_comparison.png\")\n",
    "\n",
    "# 2. Residuals plot (Crucial check for Heteroscedasticity)\n",
    "residuals = y_test - y_test_pred # Residuals on ORIGINAL scale\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.5, edgecolors='k', linewidths=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Values (Original Scale)')\n",
    "plt.ylabel('Residuals (Original Scale)')\n",
    "plt.title(f'Residual Plot ({best_model_name}) - Test Set')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('residuals_plot_tree.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: residuals_plot_tree.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 3 COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdbb8a-4bf1-4376-aa06-28a102f13eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
